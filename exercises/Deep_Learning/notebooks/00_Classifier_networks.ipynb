{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying cell types with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user scprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import scprep\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "# import os\n",
    "# import h5py\n",
    "# import sklearn\n",
    "# import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the retinal bipolar data\n",
    "\n",
    "We'll use the same retinal bipolar data you saw in preprocessing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.io.download.download_google_drive(\"1kxsMav1ly_S6pQ1vKeAtlFFW3QVvilz0\", \"shekhar_data.pkl\")\n",
    "scprep.io.download.download_google_drive(\"1J4K8bo8Pys-8xayO5vtMK3t5wJ0_TG2Y\", \"shekhar_clusters.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"shekhar_data.pkl\")\n",
    "clusters = pd.read_pickle(\"shekhar_clusters.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting data to `numpy` format\n",
    "\n",
    "Tensorflow expects data to be stored as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scprep.reduce.pca(data, n_components=100, method='dense').to_numpy()\n",
    "labels, cluster_names = pd.factorize(clusters['CELLTYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(labels))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and validation sets\n",
    "\n",
    "We'll allocate 80\\% of our data for training and 20\\% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15018, 100), (3755, 100))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first let's split our data into training and validation sets\n",
    "train_test_split = int(.8 * data.shape[0])\n",
    "\n",
    "data_training = data[:train_test_split, :]\n",
    "labels_training = labels[:train_test_split]\n",
    "data_validation = data[train_test_split:, :]\n",
    "labels_validation = labels[train_test_split:]\n",
    "data_training.shape, data_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow works with an abstract computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_3:0\", shape=(10,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# let's make an object in this graph corresponding to our first 10 points\n",
    "data_tf = tf.constant(data_training[:10, :], dtype=tf.float32)\n",
    "\n",
    "# and now their corresponding labels\n",
    "labels_tf = tf.constant(labels_training[:10], dtype=tf.int32)\n",
    "\n",
    "# look at the output\n",
    "print(labels_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 2 1 4 0 3 5]\n"
     ]
    }
   ],
   "source": [
    "# compare this to the numpy data we started with\n",
    "print(labels_training[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BC5A', 'BC1B', 'BC6', 'Rod BC', 'BC6', 'BC1B', 'BC3B', 'BC5A',\n",
       "       'Rod BC', 'Muller Glia'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and now go back to the original cluster names\n",
    "cluster_names[labels_training[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data is a np variable, with actual numbers\n",
    "data_tf is a tf variable, and is just a set of instructions, i.e. \"grab numbers from this variable and make them a constant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow's `Session`\n",
    "\n",
    "tf variables are just *instructions* for how to do computation, not the actual computations themselves\n",
    "to perform computations as instructed, we need to start a session and ask for the output by \"running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 2., 1., 4., 0., 3., 5.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(labels_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(sess.run(data_tf), data_training[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.55465302e+02, 5.86087031e+04, 4.70691553e+03, 5.71816406e+04,\n",
       "        2.26201950e+02, 1.89646558e+03, 2.08491040e+03, 9.73422229e-01,\n",
       "        2.76444971e+03, 2.08067505e+02, 1.80798450e+03, 2.04972794e+02,\n",
       "        2.91574658e+03, 5.56084277e+03, 1.89451157e+02, 4.93480377e+02,\n",
       "        8.17330933e+02, 1.20037830e+03, 5.04254181e+02, 3.29946198e+02,\n",
       "        3.34681976e+02, 1.81121780e+02, 1.18960498e+03, 1.11654945e+02,\n",
       "        5.72496729e+03, 1.48646570e+03, 1.88747818e+02, 8.97228165e+01,\n",
       "        1.19735825e+02, 1.76366806e+02, 7.00738342e+02, 3.63830902e+02,\n",
       "        1.83918190e+01, 7.48567009e+00, 1.55218701e+03, 3.49349976e+03,\n",
       "        3.63029724e+02, 7.77578857e+02, 1.76109778e+03, 6.54872192e+02,\n",
       "        4.90719109e+01, 1.01245728e+02, 1.73189758e+02, 2.09980408e+02,\n",
       "        4.55323877e+03, 2.87891455e+03, 9.27585602e+01, 1.58389636e+03,\n",
       "        5.67447021e+02, 2.02089600e+03, 3.45625758e+00, 3.18488483e+01,\n",
       "        1.56000809e+02, 1.67953014e+01, 1.14625403e+03, 2.17807593e+03,\n",
       "        6.00447937e+02, 6.25815491e+02, 9.90090408e+01, 9.55407619e-01,\n",
       "        7.53956665e+02, 4.30344092e+03, 1.51874634e+03, 1.83333289e+03,\n",
       "        9.50500305e+02, 1.07673615e+02, 3.25117310e+03, 2.94993378e+02,\n",
       "        4.45190576e+03, 1.02443260e+02, 9.00416183e+01, 1.59007660e+02,\n",
       "        8.90652561e+00, 1.90033855e+01, 3.44336456e+02, 1.60139435e+02,\n",
       "        5.88484375e+02, 9.20702148e+03, 1.65518372e+03, 9.78427429e+01,\n",
       "        1.56884403e+01, 2.30898476e+01, 1.23107307e+02, 1.21573959e+02,\n",
       "        4.77829004e+03, 1.15048608e+03, 2.10777985e+02, 1.65981360e+03,\n",
       "        3.83619531e+03, 2.55964380e+03, 3.42954369e+01, 1.59887985e+02,\n",
       "        6.39727966e+02, 1.69173088e+01, 2.49631119e+02, 2.19013580e+02,\n",
       "        1.28413843e+03, 1.66657544e+03, 8.93671570e+02, 1.42126053e+02],\n",
       "       [1.89995331e+02, 3.88662773e+04, 1.47093127e+03, 2.52823535e+04,\n",
       "        6.94031128e+02, 1.43531738e+04, 1.00647793e+04, 1.27932239e+03,\n",
       "        1.05551777e+04, 1.96395667e+03, 2.60175098e+03, 2.91062109e+03,\n",
       "        1.19358362e+03, 2.99269043e+02, 7.14104128e+00, 8.47384109e+01,\n",
       "        9.92139844e+03, 2.19360156e+03, 3.67368774e+03, 1.61423682e+03,\n",
       "        3.00448090e+02, 2.55611481e+02, 8.09193970e+02, 4.13106792e-02,\n",
       "        2.52516943e+03, 6.14945190e+02, 1.52528131e+00, 1.43675213e+01,\n",
       "        4.53621979e+01, 5.47158670e+00, 6.29856787e+03, 1.50747278e+03,\n",
       "        1.52511917e+02, 1.28439362e+02, 7.36533154e+03, 1.41315613e+03,\n",
       "        8.28292175e+02, 2.06772247e+02, 2.07348804e+03, 5.02810547e+03,\n",
       "        4.83645142e+02, 1.76055798e+03, 7.86635559e+02, 2.06218286e+03,\n",
       "        2.37974243e+03, 9.58860046e+02, 6.20963745e+02, 6.01623352e+02,\n",
       "        8.44202881e+01, 1.33722595e+03, 2.10610461e+00, 2.26738867e+03,\n",
       "        2.79442114e+03, 5.62733040e+01, 2.25399033e+02, 4.12245941e+02,\n",
       "        1.08902820e+03, 6.72161627e+00, 4.28773315e+02, 1.42877281e+00,\n",
       "        1.22622650e+02, 3.14805237e+02, 1.38888940e+03, 2.51227100e+03,\n",
       "        8.10255798e+02, 1.51906274e+03, 1.86940112e+03, 6.01786743e+02,\n",
       "        2.37798233e+02, 1.95641870e+03, 1.79239865e-02, 1.06120642e+03,\n",
       "        3.92306399e+00, 1.35431458e+03, 1.02902962e+02, 8.34679871e+02,\n",
       "        2.46680786e+02, 3.66111565e+01, 2.73824829e+02, 2.07295334e+02,\n",
       "        1.34426895e+02, 6.67524529e+00, 3.21680249e+03, 2.25396320e-01,\n",
       "        5.27550537e+02, 9.62638306e+02, 4.85391602e+02, 1.50501782e+03,\n",
       "        3.35619736e+01, 5.25324280e+02, 4.63083801e+01, 6.11908691e+02,\n",
       "        3.82055939e+02, 7.90592194e+01, 5.81870239e+02, 3.78393188e+02,\n",
       "        1.41211273e+02, 8.53719940e+01, 3.68442749e+02, 5.04422998e+03],\n",
       "       [3.23411072e+02, 1.06263584e+04, 7.36250156e+04, 2.69196911e+01,\n",
       "        3.86617266e+04, 3.12187051e+04, 2.17696509e+03, 5.01235742e+03,\n",
       "        2.12516699e+03, 1.34250806e+03, 6.41648877e+03, 1.79800625e+04,\n",
       "        1.57361975e+03, 1.65754913e+02, 2.76573486e+02, 1.19627832e+03,\n",
       "        6.16996908e+00, 3.53593521e+01, 4.29072656e+03, 4.58897070e+03,\n",
       "        5.12546936e+02, 2.68750977e+03, 3.02204492e+03, 1.69614148e+03,\n",
       "        2.13292618e+02, 2.35289383e+02, 2.92407593e+02, 1.10849902e+03,\n",
       "        3.35515625e+03, 3.43624458e+01, 4.34166351e+02, 6.56107178e+02,\n",
       "        1.15356812e+03, 7.12610657e+02, 6.16975844e-01, 1.93683020e+03,\n",
       "        6.58794189e+02, 1.31032837e+02, 8.02478333e+02, 6.79239380e+02,\n",
       "        1.12906238e+03, 3.48691504e+03, 1.42517502e+02, 7.11286743e+02,\n",
       "        3.44598503e+01, 4.80392532e+01, 1.77065332e+03, 1.69704871e+03,\n",
       "        3.87419167e+01, 6.71299011e+02, 1.42995438e+02, 2.45138623e+03,\n",
       "        3.33324805e+03, 1.31734631e+03, 1.87544678e+02, 1.02188806e+03,\n",
       "        4.26480273e+03, 1.47198779e+03, 6.58809387e+02, 6.59708313e+02,\n",
       "        5.85739929e+02, 2.32261963e+03, 9.38411743e+02, 1.43721970e+02,\n",
       "        1.33475439e+03, 1.22307892e+02, 5.45072651e+00, 7.83129395e+02,\n",
       "        1.90874985e+02, 3.44462824e+00, 2.04563293e+02, 3.74277271e+03,\n",
       "        1.74642810e+03, 1.81444427e+02, 6.87723022e+02, 5.21215248e+01,\n",
       "        7.83927612e+02, 1.74576343e+03, 1.22770593e+03, 2.38985840e+03,\n",
       "        5.12074661e+01, 5.04524231e+02, 5.72297821e+01, 2.49893188e+03,\n",
       "        9.27786926e+02, 8.01414669e-01, 1.62969894e+01, 1.67414841e+02,\n",
       "        1.21781616e+02, 1.39777970e+00, 1.12526843e+03, 8.08913269e+01,\n",
       "        2.88681030e-01, 4.57336378e+00, 2.11131024e+00, 6.76294861e+01,\n",
       "        6.88213440e+02, 3.34230194e+02, 3.31587046e-01, 2.36691711e+02],\n",
       "       [5.30658359e+04, 2.85599590e+04, 1.70979932e+03, 1.09606750e+03,\n",
       "        1.10824854e+03, 5.52343689e+02, 2.76008862e+03, 1.48005139e+03,\n",
       "        1.06066724e+03, 2.09043652e+03, 1.50691956e+03, 1.99751892e+03,\n",
       "        1.14414563e+03, 1.05402718e+02, 6.21228943e+01, 2.42964432e+02,\n",
       "        4.82577881e+03, 8.74348402e-01, 3.38398657e+03, 1.74174377e+03,\n",
       "        1.18704968e+03, 1.51294816e+00, 2.13679346e+03, 6.75308594e+02,\n",
       "        1.28397632e+03, 9.26579834e+02, 1.12402319e+03, 1.89270276e+03,\n",
       "        9.98738586e+02, 4.59030426e+02, 2.17708473e+01, 3.71414136e+03,\n",
       "        4.62558502e+02, 3.35014038e+01, 1.03797266e+03, 4.63008057e+02,\n",
       "        4.66631775e+02, 5.85725293e+03, 1.32611303e+01, 2.61464783e+02,\n",
       "        9.18229797e+02, 2.31365747e+03, 9.10458984e+02, 1.86734734e+01,\n",
       "        2.33394092e+03, 1.39038839e+01, 1.86055200e+03, 7.11401224e+00,\n",
       "        2.61878571e+02, 1.03954254e+02, 1.01219305e+03, 3.23881531e+01,\n",
       "        1.71280518e+01, 7.52216919e+02, 2.15244324e+02, 2.66272736e+02,\n",
       "        2.18192163e+03, 1.91196613e+01, 2.92050977e+03, 1.32635963e+00,\n",
       "        1.43662036e+03, 9.89825378e+02, 5.82703186e+02, 1.11214587e+03,\n",
       "        2.91174072e+02, 1.21407068e+03, 1.24363147e+03, 1.15216057e+02,\n",
       "        6.90926086e+02, 1.37340491e+03, 2.01552991e+03, 2.79826782e+02,\n",
       "        1.27061539e+02, 8.75359650e+01, 4.71967812e+01, 4.44169502e+01,\n",
       "        2.30928857e+03, 6.30979150e+03, 3.05125806e+03, 4.13018945e+03,\n",
       "        5.86648376e+02, 1.85721924e+02, 7.75513611e+02, 5.40692482e+01,\n",
       "        1.64166689e+00, 3.02847876e+03, 1.50900513e+02, 2.39598870e+00,\n",
       "        1.45010828e+03, 7.46437759e+01, 7.76666870e+02, 1.47519592e+03,\n",
       "        7.13797569e+00, 3.37074646e+02, 5.37367773e+03, 3.65714996e+02,\n",
       "        2.57802002e+02, 1.81653976e+02, 1.90070892e+02, 3.30560962e+03],\n",
       "       [2.15398535e+04, 7.70273514e+01, 1.05673643e+04, 1.19035974e+03,\n",
       "        1.10716272e+03, 2.05451992e+04, 3.35212067e+02, 1.40294905e+01,\n",
       "        3.05967188e+03, 6.73562744e+02, 4.31430634e+02, 7.08117310e+02,\n",
       "        7.02773560e+02, 1.49760239e+02, 5.13622925e+02, 8.63870972e+02,\n",
       "        2.14585596e+03, 2.91566382e+03, 7.07332568e+03, 2.30768408e+03,\n",
       "        1.68885727e+01, 1.59046341e+02, 7.13792542e+02, 2.28250305e+02,\n",
       "        6.10327209e+02, 1.37361499e+03, 1.58607349e+03, 3.48667633e+02,\n",
       "        5.88521919e+01, 1.32242584e+02, 2.81194214e+03, 3.14059296e+01,\n",
       "        1.05581519e+03, 3.70182800e+02, 2.53360352e+02, 4.45745667e+02,\n",
       "        1.63501877e+02, 2.66998828e+03, 3.17438599e+02, 6.67077100e+03,\n",
       "        2.76538428e+03, 4.12481041e+01, 1.49597925e+03, 5.23137695e+02,\n",
       "        1.06298474e+03, 2.35202393e+03, 2.68485938e+03, 1.37142590e+03,\n",
       "        3.43410889e+02, 1.36719958e+03, 9.00899963e+02, 2.28222092e+02,\n",
       "        1.47594711e+02, 6.68886602e-01, 1.03545422e+03, 2.62113647e+03,\n",
       "        2.02226257e+03, 2.25835718e+03, 1.12421516e+02, 2.81017529e+03,\n",
       "        1.09463081e+02, 1.24362965e+01, 5.36971817e+01, 1.63962646e+03,\n",
       "        5.95440857e+02, 1.16253426e+02, 1.57857141e+03, 2.20448265e+01,\n",
       "        2.50949814e+02, 2.22823145e+03, 1.36229105e+01, 1.94802444e+02,\n",
       "        1.61093506e+02, 1.46367998e+01, 6.73025274e+00, 5.88353943e+02,\n",
       "        6.46776581e+01, 1.59117249e+03, 1.12335645e+03, 2.32517219e+00,\n",
       "        1.83273962e+03, 1.63487732e+03, 5.73085754e+02, 2.05235815e+03,\n",
       "        6.96970901e+01, 3.05222144e+03, 1.57900864e+02, 7.68748108e+02,\n",
       "        2.17469391e+02, 2.92601379e+02, 9.57432079e+00, 4.53997650e+02,\n",
       "        1.87049809e+01, 3.27314362e+02, 1.49670312e+03, 1.36162451e+03,\n",
       "        9.12861145e+02, 4.99528900e+02, 6.85323563e+01, 2.48991333e+02],\n",
       "       [4.33848000e+01, 3.76063164e+04, 3.16032861e+03, 1.23252959e+04,\n",
       "        7.27958447e+03, 1.12129473e+04, 1.54533765e+03, 6.26135596e+03,\n",
       "        3.85831274e+03, 4.13604248e+03, 6.19904150e+03, 7.85179590e+03,\n",
       "        1.22900305e+03, 3.71737122e+02, 1.12162207e+04, 3.07935257e+01,\n",
       "        2.44276596e+02, 9.22100258e+00, 9.21149963e+02, 4.96540099e-01,\n",
       "        5.21505176e+03, 4.60294189e+03, 2.79595966e+01, 1.60932446e+03,\n",
       "        1.25213647e+03, 2.21770309e+02, 1.79559766e+03, 5.79089294e+02,\n",
       "        1.82353735e+03, 1.24937592e+02, 6.41613235e+01, 2.46770947e+03,\n",
       "        3.38697235e+02, 5.12714424e+01, 7.14849268e+03, 1.66238672e+03,\n",
       "        4.37207565e+01, 2.32933914e+02, 4.95114600e+03, 5.90879456e+02,\n",
       "        4.53956388e-02, 2.10722070e+03, 2.30810962e+03, 7.62087488e+00,\n",
       "        1.78806213e+03, 5.73493103e+02, 4.89126434e+02, 5.19712280e+02,\n",
       "        1.07738354e+03, 7.61316376e+01, 1.21623238e+02, 1.54526459e+02,\n",
       "        2.07423486e+03, 1.39340088e+02, 1.63262500e+03, 4.29784203e+00,\n",
       "        2.69096606e+03, 1.17890259e+03, 8.59732056e+02, 3.54622095e+03,\n",
       "        6.57750061e+02, 1.71729736e+02, 1.96464587e+03, 1.40003479e+03,\n",
       "        1.67059131e+03, 1.51509741e+03, 1.58872208e+02, 3.92831482e+02,\n",
       "        4.16939331e+02, 1.30489624e+03, 4.72921875e+02, 2.87328377e+01,\n",
       "        1.10704700e+03, 3.42589172e+02, 8.40706543e+02, 6.49428101e+01,\n",
       "        1.80776367e+01, 3.06829834e+01, 6.06141357e+02, 1.08988232e+03,\n",
       "        1.29316359e+01, 1.55616626e+03, 1.55375916e+02, 2.95452301e+02,\n",
       "        4.23935890e+01, 6.14166374e+01, 2.80006237e+01, 2.44036353e+03,\n",
       "        1.08909790e+03, 1.09619177e+03, 1.15162756e+03, 6.46725159e+02,\n",
       "        1.33936719e+03, 6.82897888e+02, 2.66128223e+03, 1.19267647e+02,\n",
       "        9.08275635e+02, 6.31827332e+02, 3.50303735e+03, 8.73654785e+02],\n",
       "       [5.43233740e+03, 4.03900898e+04, 1.11321611e+04, 8.96245544e+02,\n",
       "        1.72795703e+04, 2.29342212e+03, 1.71881152e+04, 3.43641553e+03,\n",
       "        7.89420288e+02, 1.57116760e+02, 5.04397247e+02, 5.56432227e+04,\n",
       "        2.11144141e+03, 6.51617578e+03, 4.49740771e+03, 1.12538464e+03,\n",
       "        1.83312610e+03, 4.09511383e+02, 1.95178490e+01, 4.97595042e-01,\n",
       "        1.65890030e+02, 2.39669043e+03, 8.53713928e+02, 7.28366394e+02,\n",
       "        4.28584015e+02, 2.65575378e+02, 7.31680969e+02, 2.86412476e+02,\n",
       "        1.01977704e+03, 2.43999731e+03, 7.93619263e+02, 5.16538379e+03,\n",
       "        1.08640289e+02, 1.81507095e+02, 2.92819214e+02, 8.57257767e+01,\n",
       "        1.61913223e+01, 5.28428345e+02, 1.70047571e+03, 2.17660205e+03,\n",
       "        1.67878387e+02, 4.80669678e+03, 2.34010376e+02, 2.69942212e+03,\n",
       "        3.51587250e+02, 2.88030396e+02, 4.90112000e+02, 5.24691406e+03,\n",
       "        1.93023706e+03, 9.17552673e+02, 2.29458130e+03, 4.00493546e+01,\n",
       "        1.69206641e+03, 4.00133553e+01, 2.31065247e+02, 5.56198547e+02,\n",
       "        6.30984974e+00, 1.76491650e+03, 9.91617432e+02, 1.09975708e+03,\n",
       "        5.95160095e+02, 6.05837463e+02, 5.09201622e+01, 4.27927277e+02,\n",
       "        2.12119080e+02, 1.87454675e+03, 3.12204723e+01, 2.61174243e+03,\n",
       "        8.04532288e+02, 1.84156952e+01, 4.49932281e+02, 7.92340164e+01,\n",
       "        1.05226593e+02, 4.46517395e+02, 8.60991638e+02, 4.11409187e+01,\n",
       "        2.78120312e+03, 6.34828552e+02, 2.88294554e-01, 8.52503014e+00,\n",
       "        2.55025406e+02, 3.56674385e+01, 5.92146973e+02, 2.07066309e+03,\n",
       "        1.99888229e+02, 2.92869690e+02, 1.22284338e+03, 1.01852043e+02,\n",
       "        2.81707977e+02, 2.40207593e+03, 2.62462006e+01, 6.19525795e+01,\n",
       "        1.86918110e-01, 5.60933594e+02, 1.54842300e+02, 1.25859619e+03,\n",
       "        6.16945763e+01, 6.83247223e+01, 2.27541718e+02, 8.21676731e+00],\n",
       "       [7.91487305e+02, 1.66161172e+04, 9.94807983e+02, 4.02055117e+04,\n",
       "        5.12742188e+03, 1.66416809e+02, 1.13494373e+03, 2.27656592e+03,\n",
       "        1.10478894e+03, 1.45575537e+03, 7.13758926e+01, 1.14463058e+02,\n",
       "        2.43258496e+03, 8.83190247e+02, 1.45372559e+02, 2.44652783e+03,\n",
       "        1.49889816e+02, 6.40162903e+02, 1.26172217e+03, 9.56975281e+02,\n",
       "        9.65819275e+02, 5.10130432e+02, 6.58656006e+01, 2.43138474e+02,\n",
       "        5.77352966e+02, 4.14926453e+02, 5.25926697e+02, 2.26361377e+03,\n",
       "        4.47703949e+02, 2.44419116e+03, 1.43423889e+02, 1.41959412e+03,\n",
       "        1.62785022e+03, 1.23977246e+03, 2.86614716e+02, 2.89415951e+01,\n",
       "        1.84162817e+03, 8.79922562e+01, 1.40561096e+02, 1.02802515e+03,\n",
       "        1.14623352e+03, 3.41138000e+01, 7.89480103e+02, 2.61571460e+03,\n",
       "        2.92045166e+02, 8.52452087e+00, 2.90916367e+01, 2.83550146e+03,\n",
       "        1.02292671e+01, 1.93840277e+00, 5.53880959e+01, 5.50807037e+01,\n",
       "        1.67372803e+03, 3.05837128e+02, 1.72327722e+03, 1.48497351e+03,\n",
       "        1.18700830e+03, 3.09631592e+02, 4.98787933e+02, 5.31981735e+01,\n",
       "        1.71977005e+02, 6.51222763e+01, 1.13783228e+03, 1.42826953e+03,\n",
       "        1.85718933e+02, 5.11921310e+01, 8.90005035e+01, 4.15918793e+02,\n",
       "        8.72096741e+02, 1.04944252e+02, 2.03819812e+03, 5.07512054e+02,\n",
       "        2.06265747e+03, 1.26495903e+02, 4.56804688e+03, 1.91151892e+03,\n",
       "        2.53729639e+03, 3.80303673e-02, 4.14937305e+03, 4.28195709e+02,\n",
       "        1.56144788e+03, 3.88400757e+02, 1.96080875e+01, 2.49310455e+02,\n",
       "        3.60336037e+01, 2.41928604e+02, 2.31732251e+03, 1.03220190e+03,\n",
       "        1.61432141e+03, 1.77140979e+03, 2.56715488e+00, 1.51228613e+03,\n",
       "        2.79386987e+03, 1.86782776e+03, 1.97762054e+02, 2.68135742e+02,\n",
       "        6.12810859e-03, 5.98597755e+01, 9.90445614e+00, 7.84085999e+01],\n",
       "       [3.49415664e+04, 1.66568105e+04, 5.08702911e+02, 5.79651172e+03,\n",
       "        5.67821026e+00, 1.80825830e+03, 3.06789746e+03, 1.49087537e+03,\n",
       "        2.26524634e+03, 6.35710327e+02, 1.16935327e+03, 4.16429688e+02,\n",
       "        7.22234985e+02, 8.91340820e+02, 6.47823828e+03, 2.59720142e+03,\n",
       "        2.54365039e+03, 1.84658997e+02, 8.59380531e+00, 7.86348083e+02,\n",
       "        1.83021820e+02, 5.41548096e+03, 2.10635544e+02, 6.83912659e+02,\n",
       "        9.37107520e+03, 8.96581360e+02, 3.73366165e+01, 3.94775200e+01,\n",
       "        2.45373706e+03, 3.91266174e+01, 3.67857117e+02, 2.52683687e+00,\n",
       "        3.52311426e+03, 9.27731954e-03, 2.70741974e+02, 3.18821606e+03,\n",
       "        1.09519604e+03, 5.59908104e+01, 2.75369824e+03, 1.74273849e+00,\n",
       "        1.40718823e+03, 8.20621704e+02, 4.46738464e+02, 1.02621045e+03,\n",
       "        1.79958466e+02, 9.99426575e+01, 1.08905872e+03, 3.42959412e+02,\n",
       "        3.08705884e+03, 9.61529602e+02, 2.77021362e+02, 3.21838745e+02,\n",
       "        7.05872437e+02, 2.75819325e+00, 1.06033180e+02, 1.06362228e+01,\n",
       "        5.89943733e+01, 2.99497620e+02, 2.98602238e+01, 4.57765918e+03,\n",
       "        1.27838486e+02, 1.27829871e+03, 6.14685242e+02, 4.11423187e+02,\n",
       "        1.71699905e+01, 1.13213391e+03, 1.93348328e+02, 1.81702633e+01,\n",
       "        1.34507980e+02, 2.92118988e+01, 1.13501550e+03, 4.40383339e+01,\n",
       "        9.19960022e+02, 2.96963477e+03, 3.59410248e+02, 3.00844189e+03,\n",
       "        2.13885101e+02, 2.84497772e+02, 2.59954810e+03, 1.74893539e+02,\n",
       "        7.93320862e+02, 6.02896767e+01, 3.22441559e+02, 1.03062146e+03,\n",
       "        1.17219043e+03, 8.21401215e+00, 1.24920559e+00, 7.56874939e+02,\n",
       "        5.32119691e-01, 1.29865707e+02, 4.94969139e+01, 1.03027258e+01,\n",
       "        1.21697803e+03, 7.09539368e+02, 6.49538574e+01, 1.19473314e+01,\n",
       "        2.68631256e+02, 2.14326562e+03, 1.67477161e+03, 1.69106445e+03],\n",
       "       [4.24886906e+05, 2.92754570e+04, 1.01803418e+04, 1.35418750e+03,\n",
       "        1.83796338e+03, 2.83943311e+03, 2.55679834e+03, 5.98569336e+02,\n",
       "        5.99473511e+02, 3.11342392e+01, 2.57243652e+03, 1.01482495e+03,\n",
       "        7.36521454e+01, 5.34318970e+02, 8.06897412e+03, 1.85553375e+02,\n",
       "        1.62622534e+03, 3.51437469e+02, 2.10206348e+03, 6.16833252e+02,\n",
       "        2.90168934e+01, 4.64732094e+01, 4.28259491e+02, 1.57535803e+03,\n",
       "        9.18147034e+02, 1.65417004e+03, 6.68227844e+01, 8.36229614e+02,\n",
       "        5.96567749e+02, 9.40792942e+00, 3.88095044e+03, 9.96167302e-01,\n",
       "        1.11439844e+03, 1.97442700e+03, 2.13760319e+01, 6.85393143e+00,\n",
       "        1.29890613e+03, 6.41264465e+02, 4.71892529e+03, 1.04995903e+02,\n",
       "        5.45381104e+02, 5.44209137e+01, 1.13794813e+01, 7.55823730e+02,\n",
       "        1.36207016e+02, 2.16310150e+02, 4.34474897e+00, 5.17549072e+02,\n",
       "        7.83143494e+02, 8.44544601e+01, 5.77689590e+01, 1.10682648e+02,\n",
       "        1.06855934e+02, 1.79513440e+03, 1.14032623e+02, 2.17802200e+02,\n",
       "        1.27183044e+03, 7.54244156e+01, 7.58023119e+00, 1.13496695e+01,\n",
       "        8.76704468e+02, 2.32882401e+02, 1.96934583e+03, 1.18410083e+03,\n",
       "        1.81075516e+02, 2.55716080e+02, 1.23246521e+03, 2.70893829e+02,\n",
       "        1.20629807e+02, 9.70569305e+01, 1.10377708e+02, 1.03204514e+02,\n",
       "        2.14868011e+02, 1.05537146e+03, 1.12832893e+02, 2.36369702e+03,\n",
       "        3.70457611e+01, 3.89395050e+02, 8.34037720e+02, 2.25719019e+03,\n",
       "        4.23239380e+02, 3.87915778e+00, 1.40160370e+00, 1.95761673e+02,\n",
       "        4.61055786e+02, 5.38985300e+00, 6.71992254e+00, 5.46676254e+00,\n",
       "        3.53848083e+02, 1.95417557e+02, 2.88456818e+02, 1.66229660e+02,\n",
       "        1.41819537e+00, 2.90116138e+03, 1.85151993e+02, 2.21912265e-01,\n",
       "        5.81443359e+02, 2.42363953e+02, 2.00445267e+02, 4.99432564e+01]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now give instructions for computations on this data and then ask for the output\n",
    "w = 10 * data_tf + 3\n",
    "x = w / 2\n",
    "y = x + w\n",
    "z = y**2\n",
    "\n",
    "sess.run(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note now output is a np variable that corresponds to the value of z. we do not have a np variable \n",
    "corresponding to w, x, or y. if all we want is the output z, we don't need them!\n",
    "\n",
    "## Exercise 1 - Print the last 5 rows of the data matrix with their values doubled (using tensorflow operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-60-c76805c08f8b>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-60-c76805c08f8b>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    data_last5 =\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# =================\n",
    "# Get the last five rows of `data_training`\n",
    "data_last5 = \n",
    "# Create a tensorflow constant storing `data_last5`\n",
    "tf_last5 = \n",
    "# Multiply by two\n",
    "tf_last5_double =\n",
    "# Use `sess` to compute the result\n",
    "data_last5_double =\n",
    "# Print the result\n",
    "data_last5_double\n",
    "# ================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a one-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# this function applies the simple feedforward operation\n",
    "def layer(x, n_dim, name, activation=None):\n",
    "    # create the weight matrix\n",
    "    W = tf.get_variable(dtype=tf.float32, shape=[x.get_shape()[-1], n_dim], name='W{}'.format(name))\n",
    "    # create the bias vector\n",
    "    b = tf.get_variable(dtype=tf.float32, shape=[n_dim], name='b{}'.format(name))\n",
    "    # X2 = X1 * W + b\n",
    "    output = tf.matmul(x, W) + b\n",
    "    if activation:\n",
    "        # nonlinear activation function\n",
    "        output = activation(output)\n",
    "    return output\n",
    "\n",
    "# create a hidden (middle) layer\n",
    "hidden_layer_tf = layer(data_tf, n_dim=100, name='hidden', activation=tf.nn.relu)\n",
    "\n",
    "# create the output layer used to classify\n",
    "output_tf = layer(hidden_layer_tf, n_dim=num_classes, name='output', activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a loss/score to tell our network how good or bad these results are\n",
    "# let's use cross-entropy like we talked about\n",
    "labels_one_hot = tf.one_hot(labels_tf, num_classes)\n",
    "\n",
    "loss_tf = labels_one_hot * tf.log(output_tf + 1e-6) + (1 - labels_one_hot) * tf.log(1 - output_tf + 1e-6)\n",
    "loss_tf = -1 * tf.reduce_sum(loss_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the optimizer and tell it to minimize the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need an optimizer that we'll give this loss, and it'll take responsibility\n",
    "# for updating the network to make this score go down\n",
    "learning_rate = .00001\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# this will be the tf object we call for when we want to take a single step to train our network\n",
    "train_op = opt.minimize(loss_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.89921794e-04 7.48221006e-04 1.41890964e-03 6.13534707e-04\n",
      "  4.35154624e-02 3.82874720e-02 3.49754962e-04 6.51041558e-03\n",
      "  2.25365581e-03 2.42739186e-01 4.66422699e-02 3.84765142e-03\n",
      "  1.81492288e-02 2.73553102e-04 1.07084503e-02 1.31256774e-03\n",
      "  1.29130436e-03 1.37352059e-02 1.25516031e-03 1.28589526e-01\n",
      "  1.04063285e-04 2.02990812e-03 6.55069575e-03 4.09933011e-04\n",
      "  3.28277677e-01 8.61117244e-03 8.87017474e-02 2.78331572e-03]\n",
      " [4.75706984e-05 1.44131377e-01 3.51640023e-02 7.17279863e-06\n",
      "  4.34001582e-03 8.37495027e-04 5.51097444e-04 4.35080379e-02\n",
      "  9.00734961e-03 3.05869132e-02 1.92660447e-02 2.82478362e-01\n",
      "  4.00338368e-03 7.02203065e-03 1.07017078e-03 3.25386645e-04\n",
      "  4.70022031e-04 3.01296324e-01 3.07184499e-04 1.31993205e-04\n",
      "  4.92344261e-05 1.32113113e-04 1.27230119e-03 3.66907880e-05\n",
      "  3.07156821e-03 1.90725172e-04 1.10650353e-01 4.51404703e-05]\n",
      " [4.10570588e-04 2.53576902e-04 1.76920667e-02 3.18784121e-04\n",
      "  5.08534862e-03 2.37004086e-03 3.70532594e-04 1.90910290e-03\n",
      "  1.47570961e-03 4.87927144e-04 7.01278332e-04 1.97396075e-04\n",
      "  1.34716839e-01 3.33524513e-04 1.93764567e-01 8.40394277e-05\n",
      "  8.99648740e-07 4.01658952e-01 6.90371671e-05 2.15886603e-03\n",
      "  2.47622029e-05 7.70407636e-03 1.62979087e-03 2.48675606e-05\n",
      "  2.07630187e-01 2.29343277e-04 1.72639210e-02 1.43400952e-03]\n",
      " [6.17644517e-04 1.60655391e-03 2.79646320e-03 7.09676789e-03\n",
      "  8.40680599e-01 1.53982406e-03 2.65555689e-04 6.70407200e-04\n",
      "  1.32923989e-04 3.05314927e-04 6.65135158e-04 1.14437984e-02\n",
      "  3.57429124e-02 5.19747118e-05 7.19389766e-02 2.93376943e-04\n",
      "  2.55100429e-03 4.43405879e-04 6.27203844e-03 2.57900660e-03\n",
      "  1.17673085e-03 2.89459201e-03 1.32752451e-04 5.38246427e-03\n",
      "  4.70169442e-04 4.71490901e-04 1.75171357e-03 2.64095270e-05]\n",
      " [2.67898152e-03 5.59267297e-04 3.88952927e-03 2.03877498e-05\n",
      "  7.34536648e-02 4.04830230e-03 7.05176126e-06 1.63411692e-04\n",
      "  1.58412592e-03 8.13675579e-03 2.05779706e-05 4.33899870e-04\n",
      "  2.49381825e-01 1.63037999e-04 1.97615817e-01 2.88451538e-05\n",
      "  3.53900541e-05 4.81309779e-02 1.92921177e-01 6.76935480e-04\n",
      "  1.32540567e-03 8.67068861e-03 2.70292461e-02 1.97228801e-04\n",
      "  8.17958359e-03 4.19486798e-02 1.28527001e-01 1.72225948e-04]\n",
      " [6.71114540e-05 1.70287825e-02 8.15428793e-02 1.38269952e-05\n",
      "  2.70117015e-01 1.10780122e-04 8.30697268e-03 5.85635789e-02\n",
      "  4.33097557e-05 4.15619044e-03 1.69050321e-02 4.25045602e-02\n",
      "  2.34679222e-01 4.43463475e-02 7.93268159e-03 1.28606998e-03\n",
      "  1.92299558e-05 6.41195551e-02 5.59103722e-03 1.45924538e-02\n",
      "  3.58246034e-05 4.34593717e-03 2.37282719e-02 1.24320897e-04\n",
      "  5.67527711e-02 1.58481626e-03 4.03240323e-02 1.17732561e-03]\n",
      " [1.21378707e-06 3.49041745e-02 1.25458278e-03 1.02332997e-05\n",
      "  3.04812304e-04 5.35441577e-05 1.02693157e-05 2.56559608e-04\n",
      "  1.44057731e-05 8.28988731e-01 1.13449767e-02 3.60682054e-04\n",
      "  4.70742758e-04 3.04198533e-04 1.93410615e-05 1.07672986e-06\n",
      "  2.78969583e-05 6.70292079e-02 3.58183297e-06 1.02578400e-04\n",
      "  7.93035579e-05 3.21300759e-04 1.89424227e-05 1.01969954e-06\n",
      "  3.59842218e-02 5.80209715e-04 1.63235459e-02 1.22855708e-03]\n",
      " [1.17837335e-03 1.25608430e-03 1.04090211e-03 1.28322723e-03\n",
      "  1.33890733e-01 1.07176796e-01 5.40914498e-02 6.13387255e-03\n",
      "  1.67708080e-02 2.42253207e-03 2.63513159e-02 2.76937429e-03\n",
      "  2.07494855e-01 1.56834096e-04 1.43389695e-03 5.82178880e-04\n",
      "  8.76851752e-03 7.29153007e-02 8.07393249e-03 4.80465544e-03\n",
      "  9.11320851e-04 8.85458663e-03 2.47408748e-02 1.22677349e-03\n",
      "  4.19504344e-02 1.52852722e-02 2.48179093e-01 2.55958119e-04]\n",
      " [7.36341020e-03 8.10505415e-04 1.78776309e-02 7.56184396e-04\n",
      "  1.33753680e-02 6.12210133e-04 1.69225514e-03 6.53711380e-04\n",
      "  4.31136973e-03 1.82117775e-01 1.75199867e-03 2.65848525e-02\n",
      "  3.53993654e-01 1.41502414e-02 2.91117132e-01 4.38540883e-05\n",
      "  1.12474465e-03 1.70305220e-03 3.59181431e-04 1.80638768e-02\n",
      "  1.41876210e-02 3.45813157e-03 1.24285137e-03 1.50976218e-02\n",
      "  1.38481446e-02 3.40205315e-03 2.72873510e-03 7.57184904e-03]\n",
      " [1.51955810e-05 1.29884183e-01 1.93589671e-07 8.80857254e-09\n",
      "  1.99046655e-04 2.00814251e-02 5.71901491e-03 3.78202349e-01\n",
      "  7.57446878e-06 4.76374663e-02 1.07687665e-03 1.00941653e-03\n",
      "  2.39291461e-03 7.85947428e-04 4.61878159e-07 1.72452757e-03\n",
      "  1.28308614e-07 9.32471667e-05 5.77983483e-05 2.58223875e-03\n",
      "  9.78904078e-04 1.45396043e-04 9.67211308e-05 4.81283088e-04\n",
      "  4.01783064e-02 1.29772630e-03 3.65338475e-01 1.32647592e-05]]\n"
     ]
    }
   ],
   "source": [
    "# last thing: we need to set our network weights to random values to start\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# and that's it! we've built a one-layer neural network!\n",
    "###\n",
    "####################################\n",
    "\n",
    "####################################\n",
    "### Let's train our network!\n",
    "# we've built our network, but it probably isn't very good yet\n",
    "# (it just has random values, after all)\n",
    "\n",
    "# to check that, let's feed our np data to our tf network and see how it does\n",
    "\n",
    "output_np, labels_np = sess.run([output_tf, labels_tf])\n",
    "\n",
    "print(output_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 17 17  4 12  4  9 26 12  7]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(output_np, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 2 1 4 0 3 5]\n"
     ]
    }
   ],
   "source": [
    "print(labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0 / 10\n"
     ]
    }
   ],
   "source": [
    "print('Correct: {} / {}'.format((np.argmax(output_np, axis=1) == labels_np).sum(), output_np.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not very good :(\n",
    "# but that's ok, it hasn't had a chance to train yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training step 0 correct: 0 / 10\n",
      "Training step 100 correct: 2 / 10\n",
      "Training step 200 correct: 5 / 10\n",
      "Training step 300 correct: 5 / 10\n",
      "Training step 400 correct: 9 / 10\n",
      "Training step 500 correct: 10 / 10\n",
      "Training step 600 correct: 10 / 10\n",
      "Training step 700 correct: 10 / 10\n",
      "Training step 800 correct: 10 / 10\n",
      "Training step 900 correct: 10 / 10\n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "    sess.run(train_op)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        output_np, labels_np = sess.run([output_tf, labels_tf])\n",
    "        print('Training step {} correct: {} / {}'.format(step, (np.argmax(output_np, axis=1) == labels_np).sum(), output_np.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it knows these 10 data points perfectly\n",
    "###\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start again with placeholders so we can use all of t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### tf placeholders\n",
    "# the power of tensorflow is that we are able to define computations\n",
    "# as we did above, but with 'placeholders' instead of actual data\n",
    "# we just have to define the shape and type of the variable, and then we don't\n",
    "# have to give it actual data until we call sess.run\n",
    "# then we can call the same computation over and over again with\n",
    "# different data without having to rewrite the tensorflow code\n",
    "\n",
    "# so now let's start over and do it with tf placeholders\n",
    "# conveniently, we don't have to specify the number of rows and can instead just use \"None\"\n",
    "# to indicate this may vary from batch to batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # a helpful function for clearing the tf code in your existing session\n",
    "batch_size = 10\n",
    "data_tf = tf.placeholder(shape=[None, data.shape[1]], dtype=tf.float32, name='data_tf')\n",
    "labels_tf = tf.placeholder(shape=[None], dtype=tf.int32, name='labels_tf')\n",
    "\n",
    "\n",
    "hidden_layer_tf = layer(data_tf, 10, 'hidden_layer', activation=tf.nn.relu)\n",
    "\n",
    "output_tf = layer(hidden_layer_tf, num_classes, 'output_tf', activation=tf.nn.softmax)\n",
    "\n",
    "labels_one_hot = tf.one_hot(labels_tf, num_classes)\n",
    "\n",
    "loss_tf = labels_one_hot * tf.log(output_tf + 1e-6) + (1 - labels_one_hot) * tf.log(1 - output_tf + 1e-6)\n",
    "loss_tf = - tf.reduce_sum(loss_tf)\n",
    "\n",
    "learning_rate = .001\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train_op = opt.minimize(loss_tf)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's train our network with new data each step\n",
    "step = 0\n",
    "for epoch in range(100):\n",
    "    random_order = np.random.choice(data_training.shape[0], data_training.shape[0], replace=False)\n",
    "    data_randomized = data_training[random_order]\n",
    "    labels_randomized = labels_training[random_order]\n",
    "    \n",
    "    for data_batch, labels_batch in zip(np.array_split(data_randomized, data_randomized.shape[0] // batch_size), np.array_split(labels_randomized, labels_randomized.shape[0] // batch_size)):\n",
    "        step += 1\n",
    "\n",
    "        sess.run(train_op, {data_tf: data_batch, labels_tf: labels_batch})\n",
    "\n",
    "        # evaluate accuracy on both the training and validation datasets every once in awhile\n",
    "        if step % 10 == 0:\n",
    "            loss_np = sess.run(loss_tf, {data_tf: data_batch, labels_tf: labels_batch})\n",
    "            output_np = []\n",
    "            labels_np = []\n",
    "            for data_batch, labels_batch in zip(np.array_split(data_training, data_training.shape[0] // ), np.array_split(labels_training, labels_training.shape[0] // batch_size)):\n",
    "                output_np_ = sess.run(output_tf, {data_tf: data_batch})\n",
    "                output_np.append(output_np_)\n",
    "                labels_np.append(labels_batch)\n",
    "            output_np = np.concatenate(output_np, axis=0)\n",
    "            labels_np = np.concatenate(labels_np, axis=0)\n",
    "            acc_training = (np.argmax(output_np, axis=1) == labels_np).sum() / output_np.shape[0]\n",
    "\n",
    "            output_np = []\n",
    "            labels_np = []\n",
    "            for data_batch, labels_batch in zip(np.array_split(data_validation, data_validation.shape[0] // batch_size), np.array_split(labels_validation, labels_validation.shape[0] // batch_size)):\n",
    "                output_np_ = sess.run(output_tf, {data_tf: data_batch})\n",
    "                output_np.append(output_np_)\n",
    "                labels_np.append(labels_batch)\n",
    "            output_np = np.concatenate(output_np, axis=0)\n",
    "            labels_np = np.concatenate(labels_np, axis=0)\n",
    "            acc_validation = (np.argmax(output_np, axis=1) == labels_np).sum() / output_np.shape[0] \n",
    "            print('Step {} loss: {:.3f} training accuracy: {:.3f} validation accuracy: {:.3f} '.format(step, loss_np, acc_training, acc_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-51-ef2a91e319f7>, line 118)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-51-ef2a91e319f7>\"\u001b[1;36m, line \u001b[1;32m118\u001b[0m\n\u001b[1;33m    for data_batch, labels_batch in zip(np.array_split(data_training, data_training.shape[0] // ), np.array_split(labels_training, labels_training.shape[0] // batch_size)):\u001b[0m\n\u001b[1;37m                                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# how did our network do?\n",
    "###\n",
    "####################################\n",
    "\n",
    "####################################\n",
    "### Exercise 3\n",
    "\n",
    "# create a network with a wider hidden layer and compare its performance to the network with 10 hidden neurons we just built\n",
    "\n",
    "###\n",
    "####################################\n",
    "\n",
    "####################################\n",
    "### Exercise 3\n",
    "\n",
    "# create a network with *two* hidden layers and compare its performance to the network with one hidden layer we just built\n",
    "\n",
    "###\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### Exercise 4\n",
    "\n",
    "# create a network with *five* hidden layers and compare its performance to the network with one hidden layer we just built\n",
    "\n",
    "###\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "####################################\n",
    "####################################\n",
    "###############\n",
    "###############\n",
    "###############\n",
    "# PART II: Autoencoders\n",
    "###############\n",
    "###############\n",
    "###############\n",
    "####################################\n",
    "####################################\n",
    "####################################\n",
    "\n",
    "####################################\n",
    "### let's build an autoencoder\n",
    "tf.reset_default_graph()\n",
    "batch_size = 100\n",
    "data_tf = tf.placeholder(shape=[None, data.shape[1]], dtype=tf.float32, name='data_tf')\n",
    "\n",
    "\n",
    "# layers will be input -> 100 -> 2 --> 100 -> output\n",
    "hidden_layer1_tf = layer(data_tf, 100, 'hidden_layer1', activation=tf.nn.relu)\n",
    "hidden_layer2_tf = layer(hidden_layer1_tf, 2, 'hidden_layer2', activation=None)\n",
    "hidden_layer3_tf = layer(hidden_layer2_tf, 100, 'hidden_layer3', activation=tf.nn.relu)\n",
    "output_tf = layer(hidden_layer3_tf, data.shape[1], 'output_tf', activation=None)\n",
    "\n",
    "\n",
    "# use mean-squared-error reconstruction loss\n",
    "loss_tf = tf.reduce_mean((data_tf - output_tf)**2)\n",
    "\n",
    "# this part is all the same as before\n",
    "learning_rate = .001\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train_op = opt.minimize(loss_tf)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=.1)))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(10):\n",
    "    random_order = np.random.choice(data.shape[0], data.shape[0], replace=False)\n",
    "    data_randomized = data[random_order]\n",
    "\n",
    "    for data_batch in np.array_split(data_randomized, data_randomized.shape[0] // batch_size):\n",
    "        if step % 100 == 0:\n",
    "            loss_np = sess.run(loss_tf, {data_tf: data_batch})\n",
    "            print(\"Step: {} Loss: {:.3f}\".format(step, loss_np))\n",
    "        step += 1\n",
    "\n",
    "        sess.run(train_op, {data_tf: data_batch})\n",
    "###\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### rather than evaluating our model with our data like\n",
    "### we did with the classifier, we can now use our model\n",
    "### to evaluate our data (aka exploratory data analysis)!\n",
    "\n",
    "\n",
    "# let's get the 2D internal hidden layer and visualize it\n",
    "# with a scatter plot\n",
    "viz_coordinates = []\n",
    "for data_batch in np.array_split(data, data.shape[0] // batch_size):\n",
    "    out = sess.run(hidden_layer2_tf, {data_tf: data_batch})\n",
    "    viz_coordinates.append(out)\n",
    "viz_coordinates = np.concatenate(viz_coordinates, axis=0)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1, 1)\n",
    "ax.scatter(viz_coordinates[:, 0], viz_coordinates[:, 1], c=labels, s=5)\n",
    "\n",
    "ax.set_xlabel('AE Coordinate 1')\n",
    "ax.set_ylabel('AE Coordinate 2')\n",
    "\n",
    "###\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### Exercise 4\n",
    "\n",
    "# notice we used activation=None for the hidden layer we were going to visualize\n",
    "# repeat the process with other activation functions like tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh, etc...\n",
    "# and note how the visualization changes. has the data changed at all?\n",
    "\n",
    "###\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### Exercise 5\n",
    "\n",
    "# now turn the activation for the visualization layer back to None, but experiment with\n",
    "# the activation function for the 100-dimensional layers. is there a change? why?\n",
    "\n",
    "###\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
