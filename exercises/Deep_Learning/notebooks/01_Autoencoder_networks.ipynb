{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "####################################\n",
    "####################################\n",
    "###############\n",
    "###############\n",
    "###############\n",
    "# PART II: Autoencoders\n",
    "###############\n",
    "###############\n",
    "###############\n",
    "####################################\n",
    "####################################\n",
    "####################################\n",
    "\n",
    "####################################\n",
    "### let's build an autoencoder\n",
    "tf.reset_default_graph()\n",
    "batch_size = 100\n",
    "data_tf = tf.placeholder(shape=[None, data.shape[1]], dtype=tf.float32, name='data_tf')\n",
    "\n",
    "\n",
    "# layers will be input -> 100 -> 2 --> 100 -> output\n",
    "hidden_layer1_tf = layer(data_tf, 100, 'hidden_layer1', activation=tf.nn.relu)\n",
    "hidden_layer2_tf = layer(hidden_layer1_tf, 2, 'hidden_layer2', activation=None)\n",
    "hidden_layer3_tf = layer(hidden_layer2_tf, 100, 'hidden_layer3', activation=tf.nn.relu)\n",
    "output_tf = layer(hidden_layer3_tf, data.shape[1], 'output_tf', activation=None)\n",
    "\n",
    "\n",
    "# use mean-squared-error reconstruction loss\n",
    "loss_tf = tf.reduce_mean((data_tf - output_tf)**2)\n",
    "\n",
    "# this part is all the same as before\n",
    "learning_rate = .001\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "train_op = opt.minimize(loss_tf)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=.1)))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(10):\n",
    "    random_order = np.random.choice(data.shape[0], data.shape[0], replace=False)\n",
    "    data_randomized = data[random_order]\n",
    "\n",
    "    for data_batch in np.array_split(data_randomized, data_randomized.shape[0] // batch_size):\n",
    "        if step % 100 == 0:\n",
    "            loss_np = sess.run(loss_tf, {data_tf: data_batch})\n",
    "            print(\"Step: {} Loss: {:.3f}\".format(step, loss_np))\n",
    "        step += 1\n",
    "\n",
    "        sess.run(train_op, {data_tf: data_batch})\n",
    "###\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### rather than evaluating our model with our data like\n",
    "### we did with the classifier, we can now use our model\n",
    "### to evaluate our data (aka exploratory data analysis)!\n",
    "\n",
    "\n",
    "# let's get the 2D internal hidden layer and visualize it\n",
    "# with a scatter plot\n",
    "viz_coordinates = []\n",
    "for data_batch in np.array_split(data, data.shape[0] // batch_size):\n",
    "    out = sess.run(hidden_layer2_tf, {data_tf: data_batch})\n",
    "    viz_coordinates.append(out)\n",
    "viz_coordinates = np.concatenate(viz_coordinates, axis=0)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.subplots(1, 1)\n",
    "ax.scatter(viz_coordinates[:, 0], viz_coordinates[:, 1], c=labels, s=5)\n",
    "\n",
    "ax.set_xlabel('AE Coordinate 1')\n",
    "ax.set_ylabel('AE Coordinate 2')\n",
    "\n",
    "###\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### Exercise 4\n",
    "\n",
    "# notice we used activation=None for the hidden layer we were going to visualize\n",
    "# repeat the process with other activation functions like tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh, etc...\n",
    "# and note how the visualization changes. has the data changed at all?\n",
    "\n",
    "###\n",
    "####################################\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "### Exercise 5\n",
    "\n",
    "# now turn the activation for the visualization layer back to None, but experiment with\n",
    "# the activation function for the 100-dimensional layers. is there a change? why?\n",
    "\n",
    "###\n",
    "####################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
