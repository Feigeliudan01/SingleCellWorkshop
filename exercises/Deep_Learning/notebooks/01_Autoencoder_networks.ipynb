{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis in retinal bipolar data with autoencoders\n",
    "\n",
    "In this notebook, we will build a neural network that explores the retinal bipolar dataset for Shekhar et al., 2016 without using the manually annotated cell type labels.\n",
    "\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user scprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import scprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the retinal bipolar data\n",
    "\n",
    "We'll use the same retinal bipolar data we used for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.io.download.download_google_drive(\"1pRYn62SOmmJxwVU0sSW7eBagRL2RJmx0\", \"shekhar_data.pkl\")\n",
    "scprep.io.download.download_google_drive(\"1FlNktWuJCka3pXOvNIFfRitGluZy2ftt\", \"shekhar_clusters.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle(\"shekhar_data.pkl\")\n",
    "clusters = pd.read_pickle(\"shekhar_clusters.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scprep.reduce.pca(data_raw, n_components=100, method='dense').to_numpy()\n",
    "labels, cluster_names = pd.factorize(clusters['CELLTYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building an autoencoder\n",
    "\n",
    "An **autoencoder** is a network that tries to reproduce its input. \n",
    "\n",
    "In this case, we will squeeze the data through a two-dimensional bottleneck which we can use for visualization. Also, reducing the dimension from 100 down to 2 forces the network to only retain the most important information, which intrinsically behaves as a kind of denoising.\n",
    "\n",
    "**Note Dan/Scott/Matt**: Does the term 'bottleneck' have a specific mathematical meaning.  If so that needs to be fleshed out a bit here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a `Session`\n",
    "\n",
    "You only have to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function applies the simple feedforward operation\n",
    "def layer(x, n_dim, name, activation=None):\n",
    "    # create the weight matrix\n",
    "    W = tf.get_variable(dtype=tf.float32, shape=[x.get_shape()[-1], n_dim], name='W{}'.format(name))\n",
    "    # create the bias vector\n",
    "    b = tf.get_variable(dtype=tf.float32, shape=[n_dim], name='b{}'.format(name))\n",
    "    # X2 = X1 * W + b\n",
    "    output = tf.matmul(x, W) + b\n",
    "    if activation:\n",
    "        # nonlinear activation function\n",
    "        output = activation(output)\n",
    "    return output\n",
    "\n",
    "# we'll pass 100 data points at a time through the network\n",
    "batch_size = 100\n",
    "\n",
    "# create a placeholder for the input which is the same as the output\n",
    "data_tf = tf.placeholder(shape=[None, data.shape[1]], dtype=tf.float32, name='data_tf')\n",
    "\n",
    "\n",
    "# layers will be input -> 100 -> 2 --> 100 -> output\n",
    "# first hidden layer of size 100\n",
    "hidden_layer1_tf = layer(data_tf, 100, 'hidden_layer1', activation=tf.nn.relu)\n",
    "# we won't apply a nonlinear activation to the 2D middle layer\n",
    "hidden_layer2_tf = layer(hidden_layer1_tf, 2, 'hidden_layer2', activation=None)\n",
    "# last hidden layer of size 100\n",
    "hidden_layer3_tf = layer(hidden_layer2_tf, 100, 'hidden_layer3', activation=tf.nn.relu)\n",
    "# the output should be the same size as the input\n",
    "output_tf = layer(hidden_layer3_tf, data.shape[1], 'output_tf', activation=None)\n",
    "\n",
    "\n",
    "# use mean-squared-error reconstruction loss\n",
    "loss_tf = tf.reduce_mean((data_tf - output_tf)**2)\n",
    "\n",
    "# this part is all the same as before\n",
    "learning_rate = .001\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# create an instruction to tell tf to minimize the loss\n",
    "train_op = opt.minimize(loss_tf)\n",
    "\n",
    "# initialize variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll train the network for 10 epochs\n",
    "step = 0\n",
    "for epoch in range(10):\n",
    "    # randomize the order of the data each time through\n",
    "    random_order = np.random.permutation(data.shape[0])\n",
    "    data_randomized = data[random_order]\n",
    "\n",
    "    # train the network on batches of size `batch_size`\n",
    "    for data_batch in np.array_split(data_randomized, data_randomized.shape[0] // batch_size):\n",
    "        step += 1\n",
    "\n",
    "        # update the network weights to minimize the loss\n",
    "        sess.run(train_op, {data_tf: data_batch})\n",
    "        \n",
    "        # print the loss every 100 epochs\n",
    "        if step % 100 == 0:\n",
    "            loss_np = sess.run(loss_tf, {data_tf: data_batch})\n",
    "            print(\"Step: {} Loss: {:.3f}\".format(step, loss_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the output\n",
    "\n",
    "Rather than evaluating our model with our data like we did with the classifier, we can now use our model to evaluate our data (aka exploratory data analysis)!  Autoencoder networks are very useful in exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the 2D internal hidden layer and visualize it with a scatter plot\n",
    "ae_coordinates = sess.run(hidden_layer2_tf, {data_tf: data})\n",
    "\n",
    "scprep.plot.scatter2d(ae_coordinates, c=cluster_names[labels],\n",
    "                      label_prefix='AE Coordinate ', discrete=True,\n",
    "                      legend_anchor=(1,1), figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "What do you notice about the visualization? How does this compare to the visualizations you have seen with PCA, t-SNE, UMAP and PHATE?\n",
    "\n",
    "#### _Breakpoint_  - once you get here, please help those around you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Activation functions on the visualization layer\n",
    "\n",
    "Notice we used `activation=None` for the hidden layer we were going to visualize repeat the process with other activation functions like `tf.nn.relu`, `tf.nn.sigmoid`, `tf.nn.tanh`, etc. You can see more in the [Tensorflow documentation](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/activations). \n",
    "\n",
    "Note how the visualization changes. Has the data changed at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset everything\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# ===================\n",
    "# Copy the code from above for both building the graph and training\n",
    "# Change `activation` in `hidden_layer2_tf` from `None` to one of the other options\n",
    "\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the 2D internal hidden layer and visualize it with a scatter plot\n",
    "ae_coordinates = sess.run(hidden_layer2_tf, {data_tf: data})\n",
    "\n",
    "scprep.plot.scatter2d(ae_coordinates, c=cluster_names[labels],\n",
    "                      label_prefix='AE Coordinate ', discrete=True,\n",
    "                      legend_anchor=(1,1), figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Breakpoint_  - once you get here, please help those around you!\n",
    "\n",
    "## Exercise 5 - Activation functions on the wide hidden layers\n",
    "\n",
    "Now turn the activation for the visualization layer back to None, but experiment with the activation function for the 100-dimensional layers.\n",
    "\n",
    "Is there a change? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset everything\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# ===================\n",
    "# Copy the code from above and change `activation` in `hidden_layer1_tf` and \n",
    "# `hidden_layer3_tf` from `None` to one of the other options\n",
    "\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the 2D internal hidden layer and visualize it with a scatter plot\n",
    "ae_coordinates = sess.run(hidden_layer2_tf, {data_tf: data})\n",
    "\n",
    "scprep.plot.scatter2d(ae_coordinates, c=cluster_names[labels],\n",
    "                      label_prefix='AE Coordinate ', discrete=True,\n",
    "                      legend_anchor=(1,1), figsize=(10,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
